{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/grisha.oryol/.cache/torch/hub/pytorch_vision_main\n",
      "/Users/grisha.oryol/.cache/torch/hub/pytorch_vision_main/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/grisha.oryol/.cache/torch/hub/pytorch_vision_main/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/grisha.oryol/.cache/torch/hub/pytorch_vision_main/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_vals = np.array([0.485, 0.456, 0.406])\n",
    "std_vals = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "net_name = 'efficientnet_b0'\n",
    "pretrained = torch.hub.load(\"pytorch/vision\", net_name, pretrained=True)\n",
    "pretrained.eval();\n",
    "pretrained.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_kernel(kernel_size=5, rad=1):\n",
    "    kernel = np.zeros((kernel_size, kernel_size))\n",
    "    for i in range(kernel_size):\n",
    "        for j in range(kernel_size):\n",
    "            kernel[i][j] = np.exp(- ((i - kernel_size // 2) ** 2 + (j - kernel_size // 2) ** 2) / rad ** 2)\n",
    "    kernel /= kernel.sum()\n",
    "    return kernel\n",
    "\n",
    "    \n",
    "class PreSmoother(torch.nn.Module):\n",
    "    def __init__(self, kernel_size=5, rad=1):\n",
    "        super(PreSmoother, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.smoother = torch.nn.Conv2d(1, 1, kernel_size, bias=False, padding=0)\n",
    "        self.kernel = smoothing_kernel(kernel_size, rad)\n",
    "        self.smoother.weight = torch.nn.Parameter(torch.tensor(self.kernel).unsqueeze(0).unsqueeze(0).float(), requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 1, x.shape[2],  x.shape[3])\n",
    "        smoothed = self.smoother(x)\n",
    "        smoothed = smoothed.reshape(1, -1, smoothed.shape[2],  smoothed.shape[3])\n",
    "        #smoothed = smoothed[:, :, self.kernel_size // 2: -(self.kernel_size // 2), self.kernel_size // 2: -(self.kernel_size // 2)]\n",
    "        return smoothed\n",
    "\n",
    "\n",
    "class NewNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "                self, class_idx, inp_size=224, inp=None, min_inp=-2, max_inp=2, init_ampl=0.1, greyscale=True,\n",
    "                kernel_size=5,\n",
    "                rad=1,\n",
    "                num_classes=1000,\n",
    "                period=100,\n",
    "                lr=0.00001,\n",
    "                max_freq=2,\n",
    "                corr_class_mult=1,\n",
    "    ):\n",
    "        self.min_inp = min_inp\n",
    "        self.max_inp = max_inp\n",
    "        self.inp_size = inp_size\n",
    "        self.inp_size_raw = inp_size + kernel_size - 1\n",
    "        super(NewNet, self).__init__()\n",
    "        if inp is not None:\n",
    "            self.inp = torch.nn.Parameter(inp)\n",
    "        else:\n",
    "            if greyscale:\n",
    "                one_channel = get_random_input(self.inp_size_raw, max_freq=max_freq)\n",
    "                rand_inp = init_ampl * torch.tensor(np.stack([one_channel.numpy().copy() for _ in range(3)])).unsqueeze(0)\n",
    "            else:\n",
    "                rand_inp = init_ampl * torch.tensor(np.stack([get_random_input(self.inp_size_raw, max_freq=max_freq).numpy().copy() for _ in range(3)])).unsqueeze(0)\n",
    "            self.inp = torch.nn.Parameter(rand_inp)\n",
    "        self.pretrained = pretrained\n",
    "        print(self.pretrained.classifier[1].weight)\n",
    "\n",
    "        self.pretrained.eval()\n",
    "        for param in self.pretrained.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.target = torch.zeros(num_classes).unsqueeze(0)\n",
    "        self.target[0, class_idx] = 1\n",
    "        self.class_idx = class_idx\n",
    "        \n",
    "        self.pre_smoother = PreSmoother(kernel_size, rad)\n",
    "        self.init_inp = torch.tensor(self.inp.detach()[0].numpy().copy())\n",
    "        self.init_smoothed = torch.tensor(self.pre_smoother(self.inp).detach()[0].numpy().copy())\n",
    "        \n",
    "        self.period = period\n",
    "        self.lr = lr\n",
    "        self.corr_class_mult = corr_class_mult\n",
    "        \n",
    "        self.mean_vals = torch.tensor(mean_vals).float()\n",
    "        self.std_vals = torch.tensor(std_vals).float()\n",
    "        \n",
    "        self.hparams = dict(class_idx=class_idx, \n",
    "                            inp_size=inp_size, \n",
    "                            min_inp=min_inp, \n",
    "                            max_inp=max_inp, \n",
    "                            init_ampl=init_ampl, \n",
    "                            greyscale=greyscale,\n",
    "                            kernel_size=kernel_size,\n",
    "                            rad=rad,\n",
    "                            num_classes=num_classes,\n",
    "                            lr=lr,\n",
    "                            max_freq=max_freq,\n",
    "                            corr_class_mult=corr_class_mult,\n",
    "                           )\n",
    "        \n",
    "        self.example_input_array = []\n",
    "    \n",
    "    \n",
    "    def on_train_start(self):\n",
    "        \n",
    "        self.hparams[\"gradient_clip_val\"] = self.trainer.gradient_clip_val\n",
    "        print(self.hparams)\n",
    "        self.logger.log_hyperparams(self.hparams)\n",
    "    \n",
    "        \n",
    "    def forward(self):\n",
    "        self.pre_smoother.eval()\n",
    "        self.smoothed = self.pre_smoother(self.inp)\n",
    "        \n",
    "        self.rescaled = ((self.smoothed.permute(2, 3, 0, 1) - self.mean_vals) / self.std_vals).permute(2, 3, 0, 1)\n",
    "        \n",
    "        self.pretrained.eval()\n",
    "        for param in self.pretrained.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.probs = torch.softmax(self.pretrained(self.rescaled), dim=-1)\n",
    "        return self.probs\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "\n",
    "        probs = self.forward()\n",
    "        reg = torch.nn.functional.relu(- self.inp).sum() + torch.nn.functional.relu(self.inp - 1).sum()\n",
    "        #inp_for_smoother = self.inp.reshape(-1, 1, self.inp_size,  self.inp_size)\n",
    "        mults = 1 + self.corr_class_mult * self.target\n",
    "        prob_mse = (mults * ((probs - self.target) ** 2)).sum()  #(probs * (0.5 - self.target)).sum() \n",
    "        loss = prob_mse + reg\n",
    "        \n",
    "        prob_mse_for_stopping = ((probs - self.target) ** 2).sum()\n",
    "        self.log(\"z_loss_for_stopping\", prob_mse_for_stopping, on_step=True)\n",
    "        if prob_mse_for_stopping < 0.0001:\n",
    "            self.trainer.should_stop = True\n",
    "        \n",
    "        self.tensorboard = self.logger.experiment\n",
    "        logged_scalars = {\n",
    "                \"prob_mse\": prob_mse,\n",
    "                \"reg\": reg,\n",
    "                \"total_loss\": loss\n",
    "        }\n",
    "        if self.trainer.global_step % (100 * self.period) == 0:\n",
    "            print(loss.item())\n",
    "\n",
    "        \n",
    "        self.tensorboard.add_scalars(\"0_losses\", logged_scalars,\n",
    "                global_step=self.trainer.global_step,\n",
    "                )\n",
    "        \n",
    "        self.tensorboard.add_scalar(\"0_prob_corr_class\", self.probs[0][self.class_idx],\n",
    "                global_step=self.trainer.global_step,\n",
    "                )\n",
    "        \n",
    "        self.tensorboard.add_scalar(\"mean_diff_init\", ((self.init_inp - self.inp[0]) ** 2).mean() ** 1/2, global_step=self.trainer.global_step)\n",
    "        if self.trainer.global_step % self.period == 0:\n",
    "            self.tensorboard.add_image(\"1_inp\", self.inp[0], global_step=self.trainer.global_step)\n",
    "            self.tensorboard.add_image(\"1_smoothed\", self.smoothed[0], global_step=self.trainer.global_step)\n",
    "            img_diff = normalize_for_logging((self.init_inp - self.inp[0]).abs().sum(axis=0))\n",
    "            self.tensorboard.add_image(\"2_inp_diff\", img_diff, global_step=self.trainer.global_step, dataformats=\"HW\")\n",
    "            \n",
    "            img_diff_color = normalize_for_logging((self.init_inp - self.inp[0]).abs(), eps=0)\n",
    "            self.tensorboard.add_image(\"2_inp_diff_color\", img_diff_color, global_step=self.trainer.global_step, dataformats=\"CHW\")\n",
    "            \n",
    "            smoothed_diff = normalize_for_logging((self.init_smoothed - self.smoothed[0]).abs().sum(axis=0), eps=0)\n",
    "            self.tensorboard.add_image(\"2_smoothed_diff\", smoothed_diff, global_step=self.trainer.global_step, dataformats=\"HW\")\n",
    "            \n",
    "            smoothed_diff_color = normalize_for_logging((self.init_smoothed - self.smoothed[0]).abs())\n",
    "            self.tensorboard.add_image(\"2_smoothed_diff_color\", smoothed_diff_color, global_step=self.trainer.global_step, dataformats=\"CHW\")\n",
    "             \n",
    "            fig = Figure()\n",
    "            canvas = FigureCanvas(fig)\n",
    "            ax = fig.gca()\n",
    "            ax.plot(self.target[0].detach().numpy())\n",
    "            ax.plot(probs[0].detach().numpy())\n",
    "            ax.axis('off')\n",
    "            canvas.draw()       # draw the canvas, cache the renderer\n",
    "            prob_image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
    "            prob_image = prob_image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            self.tensorboard.add_image(\"3_probs_plots\", prob_image, global_step=self.trainer.global_step, dataformats=\"HWC\")\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def on_after_backward(self):\n",
    "        \n",
    "        step = self.trainer.global_step\n",
    "        \n",
    "        if self.inp.grad is not None:\n",
    "            grad_img = model.inp.grad[0].abs()\n",
    "            \n",
    "            self.log(\"9_max_grad_orig\", grad_img.max(), on_step=True)\n",
    "            self.log(\"9_mean_grad_orig\", grad_img.mean(), on_step=True)\n",
    "            \n",
    "            \n",
    "            if self.trainer.global_step % self.period == 0:\n",
    "                grad_img_norm = 255 * grad_img / grad_img.max()\n",
    "                self.tensorboard.add_image(\"9_grad_img\", grad_img_norm, \n",
    "                                           global_step=step, dataformats=\"CHW\")\n",
    "                grads_grayscale = normalize_for_logging((model.inp.grad[0] ** 2).sum(axis=0))\n",
    "                self.tensorboard.add_image(\"9_grad_grayscale\", grads_grayscale,\n",
    "                                           global_step=step, dataformats=\"HW\")\n",
    "                self.tensorboard.add_histogram(\"9_grad_hist\", self.inp.grad, global_step=step)        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD([model.inp], lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "def normalize_for_logging(t, eps=0.001):\n",
    "    min_val = t.flatten().quantile(eps)\n",
    "    max_val = t.flatten().quantile(1 - eps)\n",
    "    return (t - min_val) / (max_val - min_val)\n",
    "    \n",
    "    \n",
    "class DummyDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, num_steps):\n",
    "        super(DummyDataset).__init__()\n",
    "        self.num_steps = num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(range(self.num_steps))    \n",
    "\n",
    "    \n",
    "def get_random_input(inp_size, max_freq=5):\n",
    "    \"\"\" \n",
    "    Generate random smooth input by combining random harmonics\n",
    "    \"\"\"\n",
    "    inp = sum([get_harmonic(Tx, Ty, inp_size) for Tx in range(max_freq) for Ty in range(max_freq)])\n",
    "    return (inp - inp.min()) / (inp.max() - inp.min())\n",
    "\n",
    "\n",
    "def get_harmonic(Tx, Ty, inp_size):\n",
    "    \"\"\" \n",
    "    Generate a random harmonic\n",
    "    \"\"\"\n",
    "    lx = torch.linspace(0, np.pi * Tx, inp_size).unsqueeze(0) + 2 * np.pi * np.random.rand()\n",
    "    ly = torch.linspace(0, np.pi * Ty, inp_size).unsqueeze(1)  + 2 * np.pi * np.random.rand()\n",
    "    return np.random.rand() * torch.sin(lx) * torch.sin(ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for init_ampl in [0.9, 0.1, 0.01]:\n",
    "    for class_idx in range(0, 1000, 100):\n",
    "        for kernel_size in [5, 9, 19]:\n",
    "            for _ in [1, 2]:\n",
    "                logger = TensorBoardLogger(\"logs_efficientnet_b0\", name=f\"{net_name}-{class_idx}\", default_hp_metric=False, log_graph=True)\n",
    "                trainer = pl.Trainer(logger=logger, max_epochs=1, gradient_clip_val=0.1)\n",
    "\n",
    "                model = NewNet(class_idx=class_idx, period=10, init_ampl=init_ampl, greyscale=True, lr=1e1, kernel_size=kernel_size, rad=kernel_size/2, corr_class_mult=10)\n",
    "                train_dataloader = torch.utils.data.DataLoader(DummyDataset(20000))\n",
    "                trainer.fit(model=model, train_dataloader=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39",
   "language": "python",
   "name": "p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
